{"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9593,"status":"ok","timestamp":1735202575451,"user":{"displayName":"rakshita naik","userId":"03917129213739012849"},"user_tz":-330},"id":"gN9T0b43iqog","outputId":"475843a2-873d-47dc-adb2-046b8e6146c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGPPRnaPg2TF"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import math\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import transforms, datasets\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMQ3dry7g74X"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6SpHR0rg_Xp"},"outputs":[],"source":["def make_divisible(value, divisor, min_value=None, round_down_protect=True):\n","    if min_value is None:\n","        min_value = divisor\n","    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n","    if round_down_protect and new_value < 0.9 * value:\n","        new_value += divisor\n","    return new_value"]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"paultimothymooney/breast-histopathology-images\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xJJEDYA2Cr3q","executionInfo":{"status":"ok","timestamp":1735188367775,"user_tz":-330,"elapsed":185082,"user":{"displayName":"rakshita naik","userId":"03917129213739012849"}},"outputId":"f29145ab-f43d-4cfa-a7fc-769dfd771014"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n","Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/breast-histopathology-images?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3.10G/3.10G [01:34<00:00, 35.3MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/paultimothymooney/breast-histopathology-images/versions/1\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"6G6Kp7ZHL4lo","outputId":"5f70a49c-97cb-4a8b-8523-49110fe0a197"},"execution_count":null,"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-660cef59-febb-4645-a188-73936e390b5e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-660cef59-febb-4645-a188-73936e390b5e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"cMTYVTX4DWs-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download -d paultimothymooney/breast-histopathology-images"],"metadata":{"id":"ssPknxurMFOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/Histo"],"metadata":{"id":"cK6e1X4cMSLN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/breast-histopathology-images.zip -d /content/Histo"],"metadata":{"id":"tEICRwqDMpdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","# Source directory containing the subfolders\n","source_dir = '/content/Histo'\n","\n","# Destination directory where you want to merge the folders\n","destination_dir = '/content/Histopathology'\n","\n","# Create the destination directories for 0 and 1 if they don't exist\n","os.makedirs(os.path.join(destination_dir, '0'), exist_ok=True)\n","os.makedirs(os.path.join(destination_dir, '1'), exist_ok=True)\n","\n","# Iterate through all subdirectories\n","for subfolder in os.listdir(source_dir):\n","    subfolder_path = os.path.join(source_dir, subfolder)\n","    if os.path.isdir(subfolder_path):  # Check if it's a directory\n","        for label in ['0', '1']:\n","            label_path = os.path.join(subfolder_path, label)\n","            if os.path.exists(label_path):\n","                # Move all files from the subfolder/label to destination_dir/label\n","                for file in os.listdir(label_path):\n","                    file_path = os.path.join(label_path, file)\n","                    dest_path = os.path.join(destination_dir, label, file)\n","                    shutil.move(file_path, dest_path)\n","\n","print(\"Folders merged successfully!\")\n"],"metadata":{"id":"N2uw9X1eOBix"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmW9lXQthpTu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import math\n","\n","\n","__all__ = ['mobilenetv4_conv_small', 'mobilenetv4_conv_medium', 'mobilenetv4_conv_large',\n","           'mobilenetv4_hybrid_medium', 'mobilenetv4_hybrid_large']\n","\n","\n","def make_divisible(value, divisor, min_value=None, round_down_protect=True):\n","    if min_value is None:\n","        min_value = divisor\n","    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if round_down_protect and new_value < 0.9 * value:\n","        new_value += divisor\n","    return new_value\n","\n","\n","class ConvBN(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n","        super(ConvBN, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride, (kernel_size - 1)//2, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class UniversalInvertedBottleneck(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 expand_ratio,\n","                 start_dw_kernel_size,\n","                 middle_dw_kernel_size,\n","                 stride,\n","                 middle_dw_downsample: bool = True,\n","                 use_layer_scale: bool = False,\n","                 layer_scale_init_value: float = 1e-5):\n","        super(UniversalInvertedBottleneck, self).__init__()\n","        self.start_dw_kernel_size = start_dw_kernel_size\n","        self.middle_dw_kernel_size = middle_dw_kernel_size\n","\n","        if start_dw_kernel_size:\n","           self.start_dw_conv = nn.Conv2d(in_channels, in_channels, start_dw_kernel_size,\n","                                          stride if not middle_dw_downsample else 1,\n","                                          (start_dw_kernel_size - 1) // 2,\n","                                          groups=in_channels, bias=False)\n","           self.start_dw_norm = nn.BatchNorm2d(in_channels)\n","\n","        expand_channels = make_divisible(in_channels * expand_ratio, 8)\n","        self.expand_conv = nn.Conv2d(in_channels, expand_channels, 1, 1, bias=False)\n","        self.expand_norm = nn.BatchNorm2d(expand_channels)\n","        self.expand_act = nn.ReLU(inplace=True)\n","\n","        if middle_dw_kernel_size:\n","           self.middle_dw_conv = nn.Conv2d(expand_channels, expand_channels, middle_dw_kernel_size,\n","                                           stride if middle_dw_downsample else 1,\n","                                           (middle_dw_kernel_size - 1) // 2,\n","                                           groups=expand_channels, bias=False)\n","           self.middle_dw_norm = nn.BatchNorm2d(expand_channels)\n","           self.middle_dw_act = nn.ReLU(inplace=True)\n","\n","        self.proj_conv = nn.Conv2d(expand_channels, out_channels, 1, 1, bias=False)\n","        self.proj_norm = nn.BatchNorm2d(out_channels)\n","\n","        if use_layer_scale:\n","            self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((out_channels)), requires_grad=True)\n","\n","        self.use_layer_scale = use_layer_scale\n","        self.identity = stride == 1 and in_channels == out_channels\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        if self.start_dw_kernel_size:\n","            x = self.start_dw_conv(x)\n","            x = self.start_dw_norm(x)\n","\n","        x = self.expand_conv(x)\n","        x = self.expand_norm(x)\n","        x = self.expand_act(x)\n","\n","        if self.middle_dw_kernel_size:\n","            x = self.middle_dw_conv(x)\n","            x = self.middle_dw_norm(x)\n","            x = self.middle_dw_act(x)\n","\n","        x = self.proj_conv(x)\n","        x = self.proj_norm(x)\n","\n","        if self.use_layer_scale:\n","            x = self.gamma * x\n","\n","        return x + shortcut if self.identity else x\n","\n","\n","class MobileNetV4(nn.Module):\n","    def __init__(self, block_specs, num_classes=1000):\n","        super(MobileNetV4, self).__init__()\n","\n","        c = 3\n","        layers = []\n","        for block_type, *block_cfg in block_specs:\n","            if block_type == 'conv_bn':\n","                block = ConvBN\n","                k, s, f = block_cfg\n","                layers.append(block(c, f, k, s))\n","            elif block_type == 'uib':\n","                block = UniversalInvertedBottleneck\n","                start_k, middle_k, s, f, e = block_cfg\n","                layers.append(block(c, f, e, start_k, middle_k, s))\n","            else:\n","                raise NotImplementedError\n","            c = f\n","        self.features = nn.Sequential(*layers)\n","        # building last several layers\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        hidden_channels = 1280\n","        self.conv = ConvBN(c, hidden_channels, 1)\n","        self.classifier = nn.Linear(hidden_channels, num_classes)\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = self.conv(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","\n","def mobilenetv4_conv_small(**kwargs):\n","    \"\"\"\n","    Constructs a MobileNetV4-Conv-Small model\n","    \"\"\"\n","    block_specs = [\n","        # conv_bn, kernel_size, stride, out_channels\n","        # uib, start_dw_kernel_size, middle_dw_kernel_size, stride, out_channels, expand_ratio\n","        # 112px\n","        ('conv_bn', 3, 2, 32),\n","        # 56px\n","        ('conv_bn', 3, 2, 32),\n","        ('conv_bn', 1, 1, 32),\n","        # 28px\n","        ('conv_bn', 3, 2, 96),\n","        ('conv_bn', 1, 1, 64),\n","        # 14px\n","        ('uib', 5, 5, 2, 96, 3.0),  # ExtraDW\n","        ('uib', 0, 3, 1, 96, 2.0),  # IB\n","        ('uib', 0, 3, 1, 96, 2.0),  # IB\n","        ('uib', 0, 3, 1, 96, 2.0),  # IB\n","        ('uib', 0, 3, 1, 96, 2.0),  # IB\n","        ('uib', 3, 0, 1, 96, 4.0),  # ConvNext\n","        # 7px\n","        ('uib', 3, 3, 2, 128, 6.0),  # ExtraDW\n","        ('uib', 5, 5, 1, 128, 4.0),  # ExtraDW\n","        ('uib', 0, 5, 1, 128, 4.0),  # IB\n","        ('uib', 0, 5, 1, 128, 3.0),  # IB\n","        ('uib', 0, 3, 1, 128, 4.0),  # IB\n","        ('uib', 0, 3, 1, 128, 4.0),  # IB\n","        ('conv_bn', 1, 1, 960),  # Conv\n","    ]\n","    return MobileNetV4(block_specs, **kwargs)\n","\n","\n","def mobilenetv4_conv_medium(**kwargs):\n","    \"\"\"\n","    Constructs a MobileNetV4-Conv-Medium model\n","    \"\"\"\n","    block_specs = [\n","        ('conv_bn', 3, 2, 32),\n","        ('conv_bn', 3, 2, 128),\n","        ('conv_bn', 1, 1, 48),\n","        # 3rd stage\n","        ('uib', 3, 5, 2, 80, 4.0),\n","        ('uib', 3, 3, 1, 80, 2.0),\n","        # 4th stage\n","        ('uib', 3, 5, 2, 160, 6.0),\n","        ('uib', 3, 3, 1, 160, 4.0),\n","        ('uib', 3, 3, 1, 160, 4.0),\n","        ('uib', 3, 5, 1, 160, 4.0),\n","        ('uib', 3, 3, 1, 160, 4.0),\n","        ('uib', 3, 0, 1, 160, 4.0),\n","        ('uib', 0, 0, 1, 160, 2.0),\n","        ('uib', 3, 0, 1, 160, 4.0),\n","        # 5th stage\n","        ('uib', 5, 5, 2, 256, 6.0),\n","        ('uib', 5, 5, 1, 256, 4.0),\n","        ('uib', 3, 5, 1, 256, 4.0),\n","        ('uib', 3, 5, 1, 256, 4.0),\n","        ('uib', 0, 0, 1, 256, 4.0),\n","        ('uib', 3, 0, 1, 256, 4.0),\n","        ('uib', 3, 5, 1, 256, 2.0),\n","        ('uib', 5, 5, 1, 256, 4.0),\n","        ('uib', 0, 0, 1, 256, 4.0),\n","        ('uib', 0, 0, 1, 256, 4.0),\n","        ('uib', 5, 0, 1, 256, 2.0),\n","        # FC layers\n","        ('conv_bn', 1, 1, 960),\n","    ]\n","\n","    return MobileNetV4(block_specs, **kwargs)\n","\n","\n","def mobilenetv4_conv_large(**kwargs):\n","    \"\"\"\n","    Constructs a MobileNetV4-Conv-Large model\n","    \"\"\"\n","    block_specs = [\n","        ('conv_bn', 3, 2, 24),\n","        ('conv_bn', 3, 2, 96),\n","        ('conv_bn', 1, 1, 48),\n","        ('uib', 3, 5, 2, 96, 4.0),\n","        ('uib', 3, 3, 1, 96, 4.0),\n","        ('uib', 3, 5, 2, 192, 4.0),\n","        ('uib', 3, 3, 1, 192, 4.0),\n","        ('uib', 3, 3, 1, 192, 4.0),\n","        ('uib', 3, 3, 1, 192, 4.0),\n","        ('uib', 3, 5, 1, 192, 4.0),\n","        ('uib', 5, 3, 1, 192, 4.0),\n","        ('uib', 5, 3, 1, 192, 4.0),\n","        ('uib', 5, 3, 1, 192, 4.0),\n","        ('uib', 5, 3, 1, 192, 4.0),\n","        ('uib', 5, 3, 1, 192, 4.0),\n","        ('uib', 3, 0, 1, 192, 4.0),\n","        ('uib', 5, 5, 2, 512, 4.0),\n","        ('uib', 5, 5, 1, 512, 4.0),\n","        ('uib', 5, 5, 1, 512, 4.0),\n","        ('uib', 5, 5, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('uib', 5, 3, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('uib', 5, 3, 1, 512, 4.0),\n","        ('uib', 5, 5, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('uib', 5, 0, 1, 512, 4.0),\n","        ('conv_bn', 1, 1, 960),\n","    ]\n","\n","    return MobileNetV4(block_specs, **kwargs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJizb4jHih1n"},"outputs":[],"source":["# data_transforms = {\n","#     'train': transforms.Compose([\n","#         transforms.Resize((224, 224)),\n","#         transforms.RandomHorizontalFlip(),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","#     ]),\n","#     'val': transforms.Compose([\n","#         transforms.Resize((224, 224)),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","#     ]),\n","# }\n","\n","# train_dataset = datasets.ImageFolder(root='/content/Histopathology', transform=data_transforms['train'])\n","# train_size = int(0.8 * len(train_dataset))\n","# val_size = len(train_dataset) - train_size\n","# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n","\n","from torch.utils.data import random_split, DataLoader\n","from torchvision import datasets, transforms\n","\n","# Define data transformations\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((128, 128)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomRotation(degrees=(-15, 15)),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomAffine(degrees=5, shear=(-5, 5)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((144, 144)),\n","        transforms.CenterCrop((128, 128)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","}\n","\n","# Load dataset without transformations\n","dataset = datasets.ImageFolder(root='/content/Histopathology')\n","\n","# Split dataset\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","# Apply transformations\n","train_dataset.dataset.transform = data_transforms['train']\n","val_dataset.dataset.transform = data_transforms['val']\n","\n","# DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BkF-tjai8zL"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lY4Dc0bQjmq7"},"outputs":[],"source":["model = mobilenetv4_conv_small(num_classes=2).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01FYLKWOjBtx"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lwh1_uT7kGgn"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct_predictions = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct_predictions += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_acc = correct_predictions.double() / len(train_loader.dataset)\n","        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n","\n","        # Validation Phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_correct_predictions = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_correct_predictions += torch.sum(preds == labels.data)\n","\n","        val_epoch_loss = val_loss / len(val_loader.dataset)\n","        val_epoch_acc = val_correct_predictions.double() / len(val_loader.dataset)\n","        print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVeGSl6xjy1Z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7eda4b2c-9d7a-4bb5-a683-582fe7644469"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Training Loss: 0.3502, Training Accuracy: 0.8503\n","Validation Loss: 0.3185, Validation Accuracy: 0.8644\n"]}],"source":["train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFu_ENaPj4H7"},"outputs":[],"source":["image_path = '/content/drive/MyDrive/dataset/1/10254_idx5_x1651_y1201_class1.png'\n","img = Image.open(image_path)\n","\n","# Apply transformations to the image\n","img_transformed = data_transforms['val'](img)\n","img_transformed = img_transformed.unsqueeze(0)  # Add batch dimension\n","img_transformed = img_transformed.to(device)\n","\n","# Make prediction\n","model.eval()\n","with torch.no_grad():\n","    output = model(img_transformed)\n","    _, predicted = torch.max(output, 1)\n","    print(f'Predicted class: {predicted.item()}')"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Training and validation accuracy values from the provided data\n","training_accuracy = [\n","    0.8659, 0.8777, 0.8876, 0.8957, 0.8890, 0.9093, 0.9108, 0.9023, 0.9259,\n","    0.9013, 0.9249, 0.9297, 0.9268, 0.9396, 0.9410, 0.9514, 0.9320, 0.9490,\n","    0.9585, 0.9533, 0.9570, 0.9622, 0.9688, 0.9769, 0.9539, 0.9660, 0.9717,\n","    0.9674, 0.9769, 0.9660\n","]\n","validation_accuracy = [\n","    0.8675, 0.8604, 0.8472, 0.8555, 0.8472, 0.8594, 0.8830, 0.8962, 0.8815,\n","    0.8868, 0.8906, 0.9075, 0.9057, 0.9158, 0.9036, 0.8999, 0.9151, 0.9038,\n","    0.8925, 0.9151, 0.8887, 0.8943, 0.9019, 0.9000, 0.9057, 0.8836, 0.8962,\n","    0.9057, 0.8981, 0.9170\n","]\n","\n","epochs = list(range(1, len(training_accuracy) + 1))\n","\n","# Plotting\n","plt.figure(figsize=(10, 6))\n","plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o')\n","plt.plot(epochs, validation_accuracy, label='Validation Accuracy', marker='x')\n","plt.title('Training and Validation Accuracy vs. Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"QqB4j3j8acU6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ptflops\n"],"metadata":{"id":"GNg8N5TYdDSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from ptflops import get_model_complexity_info\n","\n","# Define the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate the model\n","model = mobilenetv4_conv_small(num_classes=2).to(device)\n","\n","# Define the input size (e.g., for 224x224 RGB images)\n","input_size = (3, 224, 224)\n","\n","# Calculate FLOPs and number of parameters\n","flops, params = get_model_complexity_info(model, input_size, as_strings=True, print_per_layer_stat=True)\n","\n","print(f\"FLOPs: {flops}\")\n","print(f\"Parameters: {params}\")\n"],"metadata":{"id":"hEjEvDhmeGdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install imbalanced-learn\n"],"metadata":{"id":"UK_vPzWImFZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n"],"metadata":{"id":"dGBADQN6mHxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","from PIL import Image\n","import os\n","import math\n","\n","# Define data transformations\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# Load the dataset\n","train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset', transform=None)\n","\n","# Extract features and labels for SMOTE\n","features, labels = [], []\n","for img, label in train_dataset:\n","    features.append(np.array(img))  # Keep the image in its original shape (height, width, channels)\n","    labels.append(label)\n","\n","# Apply SMOTE on the flattened features\n","smote = SMOTE(random_state=42)\n","features_resampled, labels_resampled = smote.fit_resample(np.array(features).reshape(len(features), -1), np.array(labels))\n","\n","# Check the number of features after resampling\n","print(f\"Resampled features shape: {features_resampled.shape}\")\n","\n","# Ensure that the resampled data size is divisible by 224 * 224 * 3\n","# Calculate the number of elements per image\n","image_size = 224 * 224 * 3\n","\n","# Calculate the total number of elements in the resampled data\n","num_samples = features_resampled.shape[0]\n","total_elements = num_samples * image_size\n","\n","# Check if the total number of elements is divisible by the image size\n","if total_elements % image_size != 0:\n","    print(f\"Total elements {total_elements} is not divisible by {image_size}. Trimming excess data.\")\n","    trim_size = (total_elements // image_size) * image_size  # Largest divisible number of elements\n","    features_resampled = features_resampled[:trim_size]\n","\n","# Now reshape the features back into the image format (num_samples, 224, 224, 3)\n","features_resampled = features_resampled.reshape(-1, 224, 224, 3)\n","\n","# Verify the reshaped shape\n","print(f\"Reshaped features shape: {features_resampled.shape}\")\n","\n","# Recreate a PyTorch Dataset\n","class ResampledDataset(torch.utils.data.Dataset):\n","    def __init__(self, features, labels, transform=None):\n","        self.features = features\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.features[idx].astype('uint8')  # Ensure the image is in uint8 format for PIL\n","        image = Image.fromarray(image)  # Convert to PIL.Image\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)  # Apply transformations\n","        return image, label\n","\n","train_resampled_dataset = ResampledDataset(features_resampled, labels_resampled, transform=data_transforms['train'])\n","\n","# Split into training and validation sets\n","train_size = int(0.8 * len(train_resampled_dataset))\n","val_size = len(train_resampled_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_resampled_dataset, [train_size, val_size])\n","\n","# Data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Define MobileNetV4 helper functions and architecture\n","def make_divisible(value, divisor, min_value=None, round_down_protect=True):\n","    if min_value is None:\n","        min_value = divisor\n","    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if round_down_protect and new_value < 0.9 * value:\n","        new_value += divisor\n","    return new_value\n","\n","class ConvBN(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n","        super(ConvBN, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size, stride, (kernel_size - 1)//2, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","class UniversalInvertedBottleneck(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 expand_ratio,\n","                 start_dw_kernel_size,\n","                 middle_dw_kernel_size,\n","                 stride,\n","                 middle_dw_downsample: bool = True,\n","                 use_layer_scale: bool = False,\n","                 layer_scale_init_value: float = 1e-5):\n","        super(UniversalInvertedBottleneck, self).__init__()\n","        self.start_dw_kernel_size = start_dw_kernel_size\n","        self.middle_dw_kernel_size = middle_dw_kernel_size\n","\n","        if start_dw_kernel_size:\n","            self.start_dw_conv = nn.Conv2d(in_channels, in_channels, start_dw_kernel_size,\n","                                          stride if not middle_dw_downsample else 1,\n","                                          (start_dw_kernel_size - 1) // 2,\n","                                          groups=in_channels, bias=False)\n","            self.start_dw_norm = nn.BatchNorm2d(in_channels)\n","\n","        expand_channels = make_divisible(in_channels * expand_ratio, 8)\n","        self.expand_conv = nn.Conv2d(in_channels, expand_channels, 1, 1, bias=False)\n","        self.expand_norm = nn.BatchNorm2d(expand_channels)\n","        self.expand_act = nn.ReLU(inplace=True)\n","\n","        if middle_dw_kernel_size:\n","            self.middle_dw_conv = nn.Conv2d(expand_channels, expand_channels, middle_dw_kernel_size,\n","                                           stride if middle_dw_downsample else 1,\n","                                           (middle_dw_kernel_size - 1) // 2,\n","                                           groups=expand_channels, bias=False)\n","            self.middle_dw_norm = nn.BatchNorm2d(expand_channels)\n","            self.middle_dw_act = nn.ReLU(inplace=True)\n","\n","        self.proj_conv = nn.Conv2d(expand_channels, out_channels, 1, 1, bias=False)\n","        self.proj_norm = nn.BatchNorm2d(out_channels)\n","\n","        if use_layer_scale:\n","            self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((out_channels)), requires_grad=True)\n","\n","        self.use_layer_scale = use_layer_scale\n","        self.identity = stride == 1 and in_channels == out_channels\n","\n","    def forward(self, x):\n","        shortcut = x\n","\n","        if self.start_dw_kernel_size:\n","            x = self.start_dw_conv(x)\n","            x = self.start_dw_norm(x)\n","\n","        x = self.expand_conv(x)\n","        x = self.expand_norm(x)\n","        x = self.expand_act(x)\n","\n","        if self.middle_dw_kernel_size:\n","            x = self.middle_dw_conv(x)\n","            x = self.middle_dw_norm(x)\n","            x = self.middle_dw_act(x)\n","\n","        x = self.proj_conv(x)\n","        x = self.proj_norm(x)\n","\n","        if self.use_layer_scale:\n","            x = self.gamma * x\n","\n","        return x + shortcut if self.identity else x\n","\n","class MobileNetV4(nn.Module):\n","    def __init__(self, block_specs, num_classes=1000):\n","        super(MobileNetV4, self).__init__()\n","\n","        c = 3\n","        layers = []\n","        for block_type, *block_cfg in block_specs:\n","            if block_type == 'conv_bn':\n","                block = ConvBN\n","                k, s, f = block_cfg\n","                layers.append(block(c, f, k, s))\n","            elif block_type == 'uib':\n","                block = UniversalInvertedBottleneck\n","                start_k, middle_k, s, f, e = block_cfg\n","                layers.append(block(c, f, e, start_k, middle_k, s))\n","            else:\n","                raise NotImplementedError\n","            c = f\n","        self.features = nn.Sequential(*layers)\n","        # building last several layers\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        hidden_channels = 1280\n","        self.conv = ConvBN(c, hidden_channels, 1)\n","        self.classifier = nn.Linear(hidden_channels, num_classes)\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = self.conv(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","def mobilenetv4_conv_small(num_classes):\n","    \"\"\"\n","    Constructs a MobileNetV4-Conv-Small model\n","    \"\"\"\n","    block_specs = [\n","        ('conv_bn', 3, 2, 32),\n","        ('conv_bn', 3, 2, 32),\n","        ('conv_bn', 1, 1, 32),\n","        ('conv_bn', 3, 2, 96),\n","        ('conv_bn', 1, 1, 64),\n","        ('uib', 5, 5, 2, 96, 3.0),\n","        ('uib', 0, 3, 1, 96, 2.0),\n","        ('uib', 0, 3, 1, 96, 2.0),\n","        ('uib', 0, 3, 1, 96, 2.0),\n","        ('uib', 0, 3, 1, 96, 2.0),\n","        ('uib', 3, 0, 1, 96, 4.0),\n","        ('uib', 3, 3, 2, 128, 6.0),\n","        ('uib', 5, 5, 1, 128, 4.0),\n","        ('uib', 0, 5, 1, 128, 4.0),\n","        ('uib', 0, 5, 1, 128, 3.0),\n","        ('uib', 0, 3, 1, 128, 4.0),\n","        ('uib', 0, 3, 1, 128, 4.0),\n","        ('conv_bn', 1, 1, 960),\n","    ]\n","    return MobileNetV4(block_specs, num_classes)\n","\n","# Initialize the model\n","model = mobilenetv4_conv_small(num_classes=len(train_dataset.classes))\n","\n","# Print model summary\n","print(model)\n"],"metadata":{"id":"gwzHDZGjvx2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","from PIL import Image\n","import os\n","\n","# Define data transformations\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# Load the dataset\n","train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/dataset', transform=None)\n","\n","# Extract features and labels for SMOTE\n","features, labels = [], []\n","for img, label in train_dataset:\n","    features.append(np.array(img))  # Keep the image in its original shape (height, width, channels)\n","    labels.append(label)\n","\n","# Apply SMOTE on the flattened features\n","smote = SMOTE(random_state=42)\n","features_resampled, labels_resampled = smote.fit_resample(np.array(features).reshape(len(features), -1), np.array(labels))\n","\n","# Check the number of features after resampling\n","print(f\"Resampled features shape: {features_resampled.shape}\")\n","\n","# Ensure that the resampled data size is divisible by 224 * 224 * 3\n","image_size = 224 * 224 * 3\n","num_samples = features_resampled.shape[0]\n","\n","# Calculate the total number of elements in the resampled data\n","total_elements = num_samples * image_size\n","\n","# Check if total_elements is divisible by image_size\n","if total_elements % image_size != 0:\n","    # Trim any excess data that doesn't fit into the expected image size\n","    print(f\"Total elements {total_elements} is not divisible by {image_size}. Trimming excess data.\")\n","    trim_size = (total_elements // image_size) * image_size  # Largest divisible number of elements\n","    features_resampled = features_resampled[:trim_size]\n","\n","# Reshape the features back to the shape of (num_samples, 224, 224, 3)\n","features_resampled = features_resampled.reshape(-1, 224, 224, 3)\n","\n","print(\"Before SMOTE:\", Counter(labels))\n","print(\"After SMOTE:\", Counter(labels_resampled))\n","\n","# Recreate a PyTorch Dataset\n","class ResampledDataset(torch.utils.data.Dataset):\n","    def __init__(self, features, labels, transform=None):\n","        self.features = features\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.features[idx].astype('uint8')  # Ensure the image is in uint8 format for PIL\n","        image = Image.fromarray(image)  # Convert to PIL.Image\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)  # Apply transformations\n","        return image, label\n","\n","train_resampled_dataset = ResampledDataset(features_resampled, labels_resampled, transform=data_transforms['train'])\n","\n","# Split into training and validation sets\n","train_size = int(0.8 * len(train_resampled_dataset))\n","val_size = len(train_resampled_dataset) - train_size\n","train_dataset, val_dataset = random_split(train_resampled_dataset, [train_size, val_size])\n","\n","# Data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Define training function\n","def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, save_path):\n","    os.makedirs(save_path, exist_ok=True)  # Ensure the save path exists\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct_predictions = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct_predictions += torch.sum(preds == labels.data)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_acc = correct_predictions.double() / len(train_loader.dataset)\n","        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        val_correct_predictions = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item() * images.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_correct_predictions += torch.sum(preds == labels.data)\n","\n","        val_epoch_loss = val_loss / len(val_loader.dataset)\n","        val_epoch_acc = val_correct_predictions.double() / len(val_loader.dataset)\n","        print(f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_acc:.4f}\")\n","\n","        # Save the model every 5 epochs\n","        if (epoch + 1) % 5 == 0:\n","            model_save_path = f\"{save_path}/model_epoch_{epoch + 1}.pt\"\n","            torch.save(model.state_dict(), model_save_path)\n","            print(f\"Model saved at: {model_save_path}\")\n","\n","# Define model, criterion, optimizer, and device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = mobilenetv4_conv_small(num_classes=2).to(device)  # Initialize the MobileNetV4 model\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train the model\n","save_path = \"/content/drive/MyDrive/saved_models\"\n","train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30, save_path=save_path)\n"],"metadata":{"id":"MGmfzQhEmoJc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}